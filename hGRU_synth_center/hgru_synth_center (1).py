# -*- coding: utf-8 -*-
"""hGRU_synth_center.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g63Sx2THdb_7nvhh9b1-Py_OKfZKRfeA

# **Setup TPUs**
"""

# !pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl
# !pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchtext==0.10.0 -f https://download.pytorch.org/whl/cu111/torch_stable.html

# !gcloud compute tpus list --zone=us-central1-a

# tpu_ip_address='10.40.52.18'
# tpu_cores=8

# # TPU configuration
# %env XRT_TPU_CONFIG=tpu_worker;0;$tpu_ip_address:8470

# # Use bfloat16
# %env XLA_USE_BF16=1

"""# **Importing Libraries**"""

# !apt-get update && apt-get install libgl1 -y.    ## !!!!!!!!!!!

# import cv2

# !pip install -U pytorch-lightning

# pip list | grep torch

# from pytorch_lightning.utilities.xla_device_utils import XLADeviceUtils
# if XLADeviceUtils.tpu_device_exists():
import torch_xla  # noqa: F401

import torch
import torch.nn as nn
from torch.nn import functional as F
from torch.utils.data import DataLoader, Dataset, random_split

import pandas as pd
from google.cloud import storage

# from pytorch_lightning.core import LightningModule, LightningDataModule
# from pytorch_lightning.metrics.functional import accuracy
# from pytorch_lightning.trainer.trainer import Trainer

import torch_xla.core.xla_model as xm

# device = xm.xla_device()
# device

# xm.xla_device_hw(device)

"""# **Environment configuration**"""

_ = !nproc
tpu_cores = tpu_cores if 'tpu_cores' in vars() else 0
num_cpus = int(_[0])
num_gpus = torch.cuda.device_count()
device = torch.device('cuda') if num_gpus else 'cpu'

print(f'Device: {device}')
print(f'CPUs: {num_cpus}')
print(f'GPUs: {num_gpus}')
print(f'TPUs: {tpu_cores}')

"""# **Download Data**"""

from google.cloud import storage
from tqdm import tqdm
import os
import shutil

# bucket_name = 'serrelab'
# prefixes = ['prj_constancy/synthetic_data/input_train_40/', 'prj_constancy/synthetic_data/output_train_40/', \
#            'prj_constancy/synthetic_data/info_train_40/', 'prj_constancy/synthetic_data/sm_train_40/']

# download_paths = ['data/input_train/', 'data/output_train/', 'data/info_train/', 'data/sm_train/']

# storage_client = storage.Client()
# bucket = storage_client.get_bucket(bucket_name)

# for i, prefix in enumerate(prefixes):
#     print('\n###########################################')
#     print(prefix)
#     print('###########################################\n')
#     blobs = bucket.list_blobs(prefix = prefix)

#     for blob in tqdm(blobs):
#         # print(blob.name)
#         file_name = blob.name
#         blob.download_to_filename(download_paths[i] + file_name.split('/')[-1])  # Download

# for path in download_paths:
#     files_list = os.listdir(path)
#     for file in files_list:
#         os.remove(path + '/' + file)

# files_list = os.listdir('data/')
# for file in tqdm(files_list):
#     if file.startswith('sm') and file.endswith('png'):
#         # shutil.copy('data' + '/' + file, 'data' + '/sm_train/' + file)
#         os.remove('data' + '/' + file)

"""# **Dataloader**"""

# !pip install opencv-python  ## !!!!!!!!!!!!!!!

# !pip install tensorflow  # !!!!!!!!!!!!!

import tensorflow as tf

# xxx = tf.io.gfile.GFile('gs://serrelab/prj_constancy/synthetic_data/input_train_40/0.png', 'rb').read()

# xxx2 = tf.io.decode_image(xxx, channels=None, dtype=tf.dtypes.uint8, name=None, expand_animations=True)

# plt.imshow(xxx2.numpy())

import os
# os.sys.path
import torch
from torch import nn
import pytorch_lightning as pl
from torchvision import transforms
from torch.utils.data import random_split, DataLoader, Dataset
import os
import cv2
import numpy as np
import torch_xla.core.xla_model as xm

class blender_data(Dataset):
    def __init__(self, input_path, output_path, sm_path):
      self.input_path = input_path
      self.output_path = output_path
      self.sm_path = sm_path

      # Defining transforms to be applied on the data
      self.transform = transforms.Compose([transforms.ToTensor()])

    def __len__(self):
        return len(self.input_path)

    def __getitem__(self, idx):
      input_array = tf.io.gfile.GFile(self.input_path[idx], 'rb').read()
      input_array = tf.io.decode_image(input_array, channels=None, dtype=tf.dtypes.uint8, name=None, expand_animations=True)
      input_array = input_array.numpy()
    
      # input_array = cv2.imread(self.input_path[idx])
        
      input_array = np.transpose(input_array, (2, 0, 1))
      input_array = (input_array/199)

      output_array = tf.io.gfile.GFile(self.output_path[idx], 'rb').read()
      output_array = tf.io.decode_image(output_array, channels=None, dtype=tf.dtypes.uint8, name=None, expand_animations=True)
      output_array = output_array.numpy()
    
      # output_array = cv2.imread(self.output_path[idx])
        
      # sm_array = cv2.imread(self.sm_path[idx])
      # sm_array = cv2.medianBlur(sm_array, 15)
      # sm_array[sm_array>0] = 1
      # output_array = np.multiply(sm_array, output_array)
      output_array = np.transpose(output_array, (2, 0, 1))
      # output_array = output_array[128,128,:]
      output_array = (output_array/199)

    #   input_array = self.transform(input_array)
    #   output_array = self.transform(output_array)


      return torch.tensor(input_array, dtype = torch.float), torch.tensor(output_array, dtype = torch.float)
    #   return input_array, output_array
        


class blender_loader(pl.LightningDataModule):
    def __init__(self, input_file, output_file, sm_file):
        super().__init__()
          
        # Directory to load Data
        self.input_file = input_file
        self.output_file = output_file
        self.sm_file = sm_file
          
        # Defining batch size of our data
        self.batch_size = 8
          
        # Defining transforms to be applied on the data
        # self.transform = transforms.Compose([
        #     transforms.ToTensor()
        # ])
  
    # def prepare_data(self):
          # Preparing our data
          
  
    def setup(self, stage=None):
        # self.input_img_list_train = os.listdir(self.input_file + '_train_40')
        # self.output_img_list_train = os.listdir(self.output_file + '_train_40')
        # self.sm_img_list_train = os.listdir(self.sm_file + '_train_40')
        
        self.input_img_list_train = tf.io.gfile.listdir(self.input_file + '_train_40')
        self.output_img_list_train = tf.io.gfile.listdir(self.output_file + '_train_40')
        self.sm_img_list_train = tf.io.gfile.listdir(self.sm_file + '_train_40')

        self.input_img_list_train = [self.input_file + '_train_40' + '/' + path for path in self.input_img_list_train]
        self.output_img_list_train = [self.output_file + '_train_40' + '/' + path for path in self.output_img_list_train]
        self.sm_img_list_train = [self.sm_file + '_train_40' + '/' + path for path in self.sm_img_list_train]
        
        # Loading our data after 
        self.data = blender_data(self.input_img_list_train, self.output_img_list_train, self.sm_img_list_train)

        # self.sampler_train = torch.utils.data.distributed.DistributedSampler(
        #     self.data, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal()
        # )

        len_data = len(self.data)
          
        self.train_data, self.test_data = random_split(self.data, [int(len_data*0.8), len_data-int(len_data*0.8)])
        
        self.sampler_train = torch.utils.data.distributed.DistributedSampler(
            self.train_data, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal()
        )
        
        self.sampler_test = torch.utils.data.distributed.DistributedSampler(
            self.test_data, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal()
        )
  
        ########---TEST---#######################

#         self.input_img_list_test = os.listdir(self.input_file + '_train')
#         self.output_img_list_test = os.listdir(self.output_file + '_train')
#         self.sm_img_list_test = os.listdir(self.sm_file + '_train')

#         self.input_img_list_test = [self.input_file + '_train' + '/' + path for path in self.input_img_list_test]
#         self.output_img_list_test = [self.output_file + '_train' + '/' + path for path in self.output_img_list_test]
#         self.sm_img_list_test = [self.sm_file + '_train' + '/' + path for path in self.sm_img_list_test]
        
#         self.test_data = blender_data(self.input_img_list_test, self.output_img_list_test, self.sm_img_list_test)

#         self.sampler_test = torch.utils.data.distributed.DistributedSampler(
#             self.test_data, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal()
#         )
  
    def train_dataloader(self):
        
        # Generating train_dataloader
        return DataLoader(self.train_data, 
                          batch_size = self.batch_size, sampler = self.sampler_train,  drop_last = True)#, num_workers = 8)
    def val_dataloader(self):
        
        # Generating val_dataloader
        return DataLoader(self.test_data,
                          batch_size = self.batch_size, sampler = self.sampler_test, drop_last = True)#, num_workers = 8)
  
    def test_dataloader(self):
        
        # Generating test_dataloader
        return DataLoader(self.test_data,
                          batch_size = self.batch_size, drop_last = True)



# if __name__ == '__main__':
#   input_file = '/users/aarjun1/scratch/color_data/input'
#   output_file = '/users/aarjun1/scratch/color_data/output'

  # color_data = blender_loader(input_file, output_file)  
  # color_data.prepare_data()
  # color_data.setup()

  # count = 0
  # for inn, outt in color_data.train_dataloader():
  #   cv2.imwrite(f'/users/aarjun1/scratch/color_data/train_in_file{count}.png', inn.numpy()[0])
  #   cv2.imwrite(f'/users/aarjun1/scratch/color_data/train_out_file{count}.png', outt.numpy()[0])
  #   count += 1
  #   if count > 5:
  #     break

  # count = 0
  # for inn, outt in color_data.val_dataloader():
  #   cv2.imwrite(f'/users/aarjun1/scratch/color_data/val_in_file{count}.png', inn.numpy()[0])
  #   cv2.imwrite(f'/users/aarjun1/scratch/color_data/val_out_file{count}.png', outt.numpy()[0])
  #   count += 1
  #   if count > 5:
  #     break

"""# **Utils**"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import transforms, utils
import os,sys
import numpy as np

def apply_gamma(rgb, gamma="srgb"):
    """Linear to gamma rgb.
    Assume that rgb values are in the [0, 1] range (but values outside are tolerated).
    gamma can be "srgb", a real-valued exponent, or None.
    >>> apply_gamma(torch.tensor([0.5, 0.4, 0.1]).view([1, 3, 1, 1]), 0.5).view(-1)
    tensor([0.2500, 0.1600, 0.0100])
    """
    if gamma == "srgb":
        T = 0.0031308
        rgb1 = torch.max(rgb, rgb.new_tensor(T))
        return torch.where(rgb < T, 12.92 * rgb, (1.055 * torch.pow(torch.abs(rgb1), 1 / 2.4) - 0.055))
    elif gamma is None:
        return rgb
    else:
        return torch.pow(torch.max(rgb, rgb.new_tensor(0.0)), 1.0 / gamma)

def remove_gamma(rgb, gamma="srgb"):
    """Gamma to linear rgb.
    Assume that rgb values are in the [0, 1] range (but values outside are tolerated).
    gamma can be "srgb", a real-valued exponent, or None.
    >>> remove_gamma(apply_gamma(torch.tensor([0.001, 0.3, 0.4])))
    tensor([0.0010,  0.3000,  0.4000])
    >>> remove_gamma(torch.tensor([0.5, 0.4, 0.1]).view([1, 3, 1, 1]), 2.0).view(-1)
    tensor([0.2500, 0.1600, 0.0100])
    """
    if gamma == "srgb":
        T = 0.04045
        rgb1 = torch.max(rgb, rgb.new_tensor(T))
        return torch.where(rgb < T, rgb / 12.92, torch.pow(torch.abs(rgb1 + 0.055) / 1.055, 2.4))
    elif gamma is None:
        return rgb
    else:
        res = torch.pow(torch.max(rgb, rgb.new_tensor(0.0)), gamma) + \
              torch.min(rgb, rgb.new_tensor(0.0)) # very important to avoid vanishing gradients
        return res

class Conv2dPad(nn.Conv2d):
    def conv2d_forward(self, input, weight):
        if self.padding_mode == "zeros":
            padding_mode = "constant"
        else:
            padding_mode = self.padding_mode
        expanded_padding = (
            self.padding[1],
            self.padding[1],
            self.padding[0],
            self.padding[0],
        )
        return F.conv2d(
            F.pad(input, expanded_padding, mode=padding_mode),
            weight,
            self.bias,
            self.stride,
            (0, 0),
            self.dilation,
            self.groups,
        )

"""# **Model**"""

import torch.nn as nn
import torch.nn.functional as F
import torch
import numpy as np
from torch.nn import init
from torch.nn import init
from torch.autograd import Function

from torchvision import datasets, models, transforms

class L1(torch.nn.Module):
    def __init__(self, module, weight_decay):
        super().__init__()
        self.module = module
        self.weight_decay = weight_decay

        # Backward hook is registered on the specified module
        self.hook = self.module.register_backward_hook(self._weight_decay_hook)

    # Not dependent on backprop incoming values, placeholder
    def _weight_decay_hook(self, *_):
        for param in self.module.parameters():
            # If there is no gradient or it was zeroed out
            # Zeroed out using optimizer.zero_grad() usually
            # Turn on if needed with grad accumulation/more safer way
            # if param.grad is None or torch.all(param.grad == 0.0):

            # Apply regularization on it
            param.grad = self.regularize(param)

    def regularize(self, parameter):
        # L1 regularization formula
        return self.weight_decay * torch.sign(parameter.data)

    def forward(self, *args, **kwargs):
        # Simply forward and args and kwargs to module
        return self.module(*args, **kwargs)


class dummyhgru(Function):
    @staticmethod
    def forward(ctx, state_2nd_last, last_state, *args):
        ctx.save_for_backward(state_2nd_last, last_state)
        ctx.args = args
        return last_state

    @staticmethod
    def backward(ctx, grad):
        neumann_g = neumann_v = None
        neumann_g_prev = grad.clone()
        neumann_v_prev = grad.clone()

        state_2nd_last, last_state = ctx.saved_tensors
        
        args = ctx.args
        truncate_iter = args[-1]
        # exp_name = args[-2]
        # i = args[-3]
        # epoch = args[-4]

        normsv = []
        normsg = []
        normg = torch.norm(neumann_g_prev)
        # normsg.append(normg.data.item())
        # normsv.append(normg.data.item())
        prev_normv = 1e8
        for ii in range(truncate_iter):
            neumann_v = torch.autograd.grad(last_state, state_2nd_last, grad_outputs=neumann_v_prev,
                                            retain_graph=True, allow_unused=True)
            normv = torch.norm(neumann_v[0])
            neumann_g = neumann_g_prev + neumann_v[0]
            normg = torch.norm(neumann_g)
            
            if normg > 1 or normv > prev_normv or normv < 1e-9:
                # normsg.append(normg.data.item())
                # normsv.append(normv.data.item())
                neumann_g = neumann_g_prev
                break

            prev_normv = normv
            neumann_v_prev = neumann_v
            neumann_g_prev = neumann_g
            
            # normsv.append(normv.data.item())
            # normsg.append(normg.data.item())
        return (None, neumann_g, None, None, None, None)

class hConvGRUCell(nn.Module):
    """
    Generate a convolutional GRU cell
    """

    def __init__(self, input_size, hidden_size, kernel_size, timesteps, batchnorm=True, grad_method='bptt', use_attention=False, \
                no_inh=False, lesion_alpha=False, lesion_gamma=False, lesion_mu=False, lesion_kappa=False, att_nl=torch.sigmoid, l1=0.):
        super(hConvGRUCell, self).__init__()
        self.padding = kernel_size // 2
        self.hidden_size = hidden_size
        self.input_size = input_size
        self.batchnorm = batchnorm
        self.timesteps = timesteps
        self.use_attention = use_attention
        self.no_inh = no_inh
        self.att_nl = att_nl
        self.grad_method = grad_method
        
        if self.use_attention:
            self.a_w_gate = nn.Conv2d(hidden_size, hidden_size, 1, padding=1 // 2)
            self.a_u_gate = nn.Conv2d(hidden_size, hidden_size, 1, padding=1 // 2)
            init.orthogonal_(self.a_w_gate.weight)
            init.orthogonal_(self.a_u_gate.weight)
            init.constant_(self.a_w_gate.bias, 1.)  # In future try setting to -1 -- originally set to 1
            init.constant_(self.a_u_gate.bias, 1.)

        self.i_w_gate = nn.Conv2d(hidden_size, hidden_size, 1)
        self.i_u_gate = nn.Conv2d(hidden_size, hidden_size, 1)
        
        self.e_w_gate = nn.Conv2d(hidden_size, hidden_size, 1)
        self.e_u_gate = nn.Conv2d(hidden_size, hidden_size, 1)

        self.exc_init = nn.Conv2d(hidden_size, hidden_size, 1)
        self.inh_init = nn.Conv2d(hidden_size, hidden_size, 1)

        spatial_h_size = kernel_size
        self.h_padding = spatial_h_size // 2
        # self.w_exc = nn.Parameter(torch.empty(hidden_size, hidden_size, spatial_h_size, spatial_h_size))
        # init.orthogonal_(self.w_exc)
        # if not no_inh:
        #     self.w_inh = nn.Parameter(torch.empty(hidden_size, hidden_size, spatial_h_size, spatial_h_size))
        #     init.orthogonal_(self.w_inh)
        self.w_exc = nn.Conv2d(hidden_size, hidden_size, spatial_h_size, padding=spatial_h_size//2)
        init.orthogonal_(self.w_exc.weight)
        if l1:
            self.w_exc = L1(self.w_exc, weight_decay=l1)
        if not no_inh:
            self.w_inh = nn.Conv2d(hidden_size, hidden_size, spatial_h_size, padding=spatial_h_size//2)
            init.orthogonal_(self.w_inh.weight) 
            if l1:
                self.w_inh = L1(self.w_inh, weight_decay=l1)

        self.alpha = nn.Parameter(torch.empty((hidden_size, 1, 1)))
        self.mu = nn.Parameter(torch.empty((hidden_size, 1, 1)))

        self.gamma = nn.Parameter(torch.empty((hidden_size, 1, 1)))
        self.kappa = nn.Parameter(torch.empty((hidden_size, 1, 1)))
        self.w = nn.Parameter(torch.empty((hidden_size, 1, 1)))

        self.bn = nn.ModuleList([nn.BatchNorm2d(hidden_size, eps=1e-03, affine=True, track_running_stats=False) for i in range(2)])
        self.bn_r = nn.ModuleList([nn.BatchNorm2d(hidden_size, eps=1e-03, affine=True, track_running_stats=False) for i in range(3)])
        # self.bn = nn.ModuleList([nn.GroupNorm(5, hidden_size, affine=True) for i in range(2)])
        self.gbn = nn.ModuleList([nn.GroupNorm(hidden_size//4, hidden_size, affine=True) for i in range(3)]) #  nn.BatchNorm2d(hidden_size, eps=1e-03, affine=True, track_running_stats=False) for i in range(2)])
        self.gbn_r = nn.ModuleList([nn.GroupNorm(hidden_size//4, hidden_size, affine=True) for i in range(2)])

        init.orthogonal_(self.i_w_gate.weight)
        init.orthogonal_(self.i_u_gate.weight)
        init.orthogonal_(self.e_w_gate.weight)
        init.orthogonal_(self.e_u_gate.weight)
        
        for bn, gbn, bn_r, gbn_r in zip(self.bn, self.gbn, self.bn_r, self.gbn_r):
            init.constant_(bn.weight, 0.1)
            init.constant_(gbn.weight, 0.1)
            init.constant_(bn_r.weight, 0.1)
            init.constant_(gbn_r.weight, 0.1)

        if not no_inh:
            init.constant_(self.alpha, 1.)
            init.constant_(self.mu, 0.)
        # init.constant_(self.alpha, 0.1)
        # init.constant_(self.mu, 1)
        init.constant_(self.gamma, 0.)
        # init.constant_(self.w, 1.)
        init.constant_(self.kappa, 1.)

        if self.use_attention:
            self.i_w_gate.bias.data = -self.a_w_gate.bias.data
            self.e_w_gate.bias.data = -self.a_w_gate.bias.data
            self.i_u_gate.bias.data = -self.a_u_gate.bias.data
            self.e_u_gate.bias.data = -self.a_u_gate.bias.data
        else:
            init.uniform_(self.i_w_gate.bias.data, 1, self.timesteps - 1)
            self.i_w_gate.bias.data.log()
            self.i_u_gate.bias.data.log()
            self.e_w_gate.bias.data = -self.i_w_gate.bias.data
            self.e_u_gate.bias.data = -self.i_u_gate.bias.data
        if lesion_alpha:
            self.alpha.requires_grad = False
            self.alpha.weight = 0.
        if lesion_mu:
            self.mu.requires_grad = False
            self.mu.weight = 0.
        if lesion_gamma:
            self.gamma.requires_grad = False
            self.gamma.weight = 0.
        if lesion_kappa:
            self.kappa.requires_grad = False
            self.kappa.weight = 0.

#     def forward(self, input_, inhibition, excitation,  activ=F.softplus, testmode=False):  # Worked with tanh and softplus
#         # Attention gate: filter input_ and excitation
#         # att_gate = torch.sigmoid(self.a_w_gate(input_) + self.a_u_gate(excitation))  # Attention Spotlight
#         # att_gate = torch.sigmoid(self.a_w_gate(input_) * self.a_u_gate(excitation))  # Attention Spotlight
#         if inhibition is None:
#             inhibition = self.inh_init(input_)
#         if excitation is None:
#             excitation = self.exc_init(input_)
#         if self.use_attention:
#             # att_gate = torch.sigmoid(self.a_w_gate(inhibition) + self.a_u_gate(excitation))  # Attention Spotlight -- MOST RECENT WORKING
#             att_gate = self.att_nl(self.gbn[0](self.a_w_gate(input_) + self.a_u_gate(excitation)))  # Attention Spotlight -- MOST RECENT WORKING
#         elif not self.use_attention and testmode:
#             att_gate = torch.zeros_like(input_)

#         # Gate E/I with attention immediately
#         if self.use_attention:
#             gated_input = input_  # * att_gate  # In activ range
#             gated_excitation = att_gate * excitation  # att_gate * excitation
#             gated_inhibition = inhibition
#             # gated_inhibition = inhibition
#         else:
#             gated_input = input_
#             gated_excitation = excitation
#             gated_inhibition = inhibition

#         if not self.no_inh:
#             # Compute inhibition
#             # inh_intx = self.bn[0](F.conv2d(gated_excitation, self.w_inh, padding=self.h_padding))  # in activ range
#             inh_intx = self.bn[0](self.w_inh(gated_excitation))  # , self.w_inh, padding=self.h_padding))  # in activ range
#             inhibition_hat = activ(input_ - inh_intx * (self.alpha * gated_inhibition + self.mu))
#             # inhibition_hat = activ(input_ - activ(inh_intx * (self.alpha * gated_inhibition + self.mu)))

#             # Integrate inhibition
#             inh_gate = torch.sigmoid(self.gbn[1](self.i_w_gate(gated_input) + self.i_u_gate(gated_inhibition)))
#             inhibition = (1 - inh_gate) * inhibition + inh_gate * inhibition_hat  # In activ range
#         else:
#             inhibition, gated_inhibition = gated_excitation, excitation

#         # Pass to excitatory neurons
#         # exc_gate = torch.sigmoid(self.e_w_gate(inhibition) + self.e_u_gate(excitation))
#         exc_gate = torch.sigmoid(self.gbn[2](self.e_w_gate(gated_inhibition) + self.e_u_gate(gated_excitation)))
#         # exc_intx = self.bn[1](F.conv2d(inhibition, self.w_exc, padding=self.h_padding))  # In activ range
#         exc_intx = self.bn[1](self.w_exc(inhibition))
#         excitation_hat = activ(inhibition + exc_intx * (self.kappa * inhibition + self.gamma))  # Skip connection OR add OR add by self-sim

#         excitation = (1 - exc_gate) * excitation + exc_gate * excitation_hat
#         if testmode:
#             return inhibition, excitation, att_gate
#         else:
#             return inhibition, excitation
    
    ################################################################
    ##################### No group norm ############################
    ################################################################
    # def forward(self, input_, inhibition, excitation,  activ=F.softplus, testmode=False):  # Worked with tanh and softplus
    #     # Attention gate: filter input_ and excitation
    #     # att_gate = torch.sigmoid(self.a_w_gate(input_) + self.a_u_gate(excitation))  # Attention Spotlight
    #     # att_gate = torch.sigmoid(self.a_w_gate(input_) * self.a_u_gate(excitation))  # Attention Spotlight
    #     if inhibition is None:
    #         inhibition = self.inh_init(input_)
    #     if excitation is None:
    #         excitation = self.exc_init(input_)
    #     if self.use_attention:
    #         # att_gate = torch.sigmoid(self.a_w_gate(inhibition) + self.a_u_gate(excitation))  # Attention Spotlight -- MOST RECENT WORKING
    #         att_gate = self.att_nl(self.a_w_gate(input_) + self.a_u_gate(excitation))  # Attention Spotlight -- MOST RECENT WORKING
    #     elif not self.use_attention and testmode:
    #         att_gate = torch.zeros_like(input_)

    #     # Gate E/I with attention immediately
    #     if self.use_attention:
    #         gated_input = input_  # * att_gate  # In activ range
    #         gated_excitation = att_gate * excitation  # att_gate * excitation
    #         gated_inhibition = inhibition
    #         # gated_inhibition = inhibition
    #     else:
    #         gated_input = input_
    #         gated_excitation = excitation
    #         gated_inhibition = inhibition

    #     if not self.no_inh:
    #         # Compute inhibition
    #         # inh_intx = self.bn[0](F.conv2d(gated_excitation, self.w_inh, padding=self.h_padding))  # in activ range
    #         inh_intx = self.bn[0](self.w_inh(gated_excitation))  # , self.w_inh, padding=self.h_padding))  # in activ range
    #         inhibition_hat = activ(input_ - inh_intx * (self.alpha * gated_inhibition + self.mu))
    #         # inhibition_hat = activ(input_ - activ(inh_intx * (self.alpha * gated_inhibition + self.mu)))

    #         # Integrate inhibition
    #         inh_gate = torch.sigmoid(self.i_w_gate(gated_input) + self.i_u_gate(gated_inhibition))
    #         inhibition = (1 - inh_gate) * inhibition + inh_gate * inhibition_hat  # In activ range
    #     else:
    #         inhibition, gated_inhibition = gated_excitation, excitation

    #     # Pass to excitatory neurons
    #     # exc_gate = torch.sigmoid(self.e_w_gate(inhibition) + self.e_u_gate(excitation))
    #     exc_gate = torch.sigmoid(self.e_w_gate(gated_inhibition) + self.e_u_gate(gated_excitation))
    #     # exc_intx = self.bn[1](F.conv2d(inhibition, self.w_exc, padding=self.h_padding))  # In activ range
    #     exc_intx = self.bn[1](self.w_exc(inhibition))
    #     excitation_hat = activ(exc_intx * (self.kappa * inhibition + self.gamma))  # Skip connection OR add OR add by self-sim

    #     excitation = (1 - exc_gate) * excitation + exc_gate * excitation_hat
    #     if testmode:
    #         return inhibition, excitation, att_gate
    #     else:
    #         return inhibition, excitation

    ################################################################
    ############ Replace all group norm with Batch norm ############
    ################################################################
    # def forward(self, input_, inhibition, excitation,  activ=F.softplus, testmode=False):  # Worked with tanh and softplus
    #     # Attention gate: filter input_ and excitation
    #     # att_gate = torch.sigmoid(self.a_w_gate(input_) + self.a_u_gate(excitation))  # Attention Spotlight
    #     # att_gate = torch.sigmoid(self.a_w_gate(input_) * self.a_u_gate(excitation))  # Attention Spotlight
    #     if inhibition is None:
    #         inhibition = self.inh_init(input_)
    #     if excitation is None:
    #         excitation = self.exc_init(input_)
    #     if self.use_attention:
    #         # att_gate = torch.sigmoid(self.a_w_gate(inhibition) + self.a_u_gate(excitation))  # Attention Spotlight -- MOST RECENT WORKING
    #         att_gate = self.att_nl(self.bn_r[0](self.a_w_gate(input_) + self.a_u_gate(excitation)))  # Attention Spotlight -- MOST RECENT WORKING
    #     elif not self.use_attention and testmode:
    #         att_gate = torch.zeros_like(input_)

    #     # Gate E/I with attention immediately
    #     if self.use_attention:
    #         gated_input = input_  # * att_gate  # In activ range
    #         gated_excitation = att_gate * excitation  # att_gate * excitation
    #         gated_inhibition = inhibition
    #         # gated_inhibition = inhibition
    #     else:
    #         gated_input = input_
    #         gated_excitation = excitation
    #         gated_inhibition = inhibition

    #     if not self.no_inh:
    #         # Compute inhibition
    #         # inh_intx = self.bn[0](F.conv2d(gated_excitation, self.w_inh, padding=self.h_padding))  # in activ range
    #         inh_intx = self.bn[0](self.w_inh(gated_excitation))  # , self.w_inh, padding=self.h_padding))  # in activ range
    #         inhibition_hat = activ(input_ - inh_intx * (self.alpha * gated_inhibition + self.mu))
    #         # inhibition_hat = activ(input_ - activ(inh_intx * (self.alpha * gated_inhibition + self.mu)))

    #         # Integrate inhibition
    #         inh_gate = torch.sigmoid(self.bn_r[1](self.i_w_gate(gated_input) + self.i_u_gate(gated_inhibition)))
    #         inhibition = (1 - inh_gate) * inhibition + inh_gate * inhibition_hat  # In activ range
    #     else:
    #         inhibition, gated_inhibition = gated_excitation, excitation

    #     # Pass to excitatory neurons
    #     # exc_gate = torch.sigmoid(self.e_w_gate(inhibition) + self.e_u_gate(excitation))
    #     exc_gate = torch.sigmoid(self.bn_r[2](self.e_w_gate(gated_inhibition) + self.e_u_gate(gated_excitation)))
    #     # exc_intx = self.bn[1](F.conv2d(inhibition, self.w_exc, padding=self.h_padding))  # In activ range
    #     exc_intx = self.bn[1](self.w_exc(inhibition))
    #     excitation_hat = activ(inhibition + exc_intx * (self.kappa * inhibition + self.gamma))  # Skip connection OR add OR add by self-sim

    #     excitation = (1 - exc_gate) * excitation + exc_gate * excitation_hat
    #     if testmode:
    #         return inhibition, excitation, att_gate
    #     else:
    #         return inhibition, excitation

    ################################################################
    ########################### No Norm ############################
    ################################################################
    # def forward(self, input_, inhibition, excitation,  activ=F.softplus, testmode=False):  # Worked with tanh and softplus
    #     # Attention gate: filter input_ and excitation
    #     # att_gate = torch.sigmoid(self.a_w_gate(input_) + self.a_u_gate(excitation))  # Attention Spotlight
    #     # att_gate = torch.sigmoid(self.a_w_gate(input_) * self.a_u_gate(excitation))  # Attention Spotlight
    #     if inhibition is None:
    #         inhibition = self.inh_init(input_)
    #     if excitation is None:
    #         excitation = self.exc_init(input_)
    #     if self.use_attention:
    #         # att_gate = torch.sigmoid(self.a_w_gate(inhibition) + self.a_u_gate(excitation))  # Attention Spotlight -- MOST RECENT WORKING
    #         att_gate = self.att_nl(self.a_w_gate(input_) + self.a_u_gate(excitation))  # Attention Spotlight -- MOST RECENT WORKING
    #     elif not self.use_attention and testmode:
    #         att_gate = torch.zeros_like(input_)

    #     # Gate E/I with attention immediately
    #     if self.use_attention:
    #         gated_input = input_  # * att_gate  # In activ range
    #         gated_excitation = att_gate * excitation  # att_gate * excitation
    #         gated_inhibition = inhibition
    #         # gated_inhibition = inhibition
    #     else:
    #         gated_input = input_
    #         gated_excitation = excitation
    #         gated_inhibition = inhibition

    #     if not self.no_inh:
    #         # Compute inhibition
    #         # inh_intx = self.bn[0](F.conv2d(gated_excitation, self.w_inh, padding=self.h_padding))  # in activ range
    #         inh_intx = self.w_inh(gated_excitation)  # , self.w_inh, padding=self.h_padding))  # in activ range
    #         inhibition_hat = activ(input_ - inh_intx * (self.alpha * gated_inhibition + self.mu))
    #         # inhibition_hat = activ(input_ - activ(inh_intx * (self.alpha * gated_inhibition + self.mu)))

    #         # Integrate inhibition
    #         inh_gate = torch.sigmoid(self.i_w_gate(gated_input) + self.i_u_gate(gated_inhibition))
    #         inhibition = (1 - inh_gate) * inhibition + inh_gate * inhibition_hat  # In activ range
    #     else:
    #         inhibition, gated_inhibition = gated_excitation, excitation

    #     # Pass to excitatory neurons
    #     # exc_gate = torch.sigmoid(self.e_w_gate(inhibition) + self.e_u_gate(excitation))
    #     exc_gate = torch.sigmoid(self.e_w_gate(gated_inhibition) + self.e_u_gate(gated_excitation))
    #     # exc_intx = self.bn[1](F.conv2d(inhibition, self.w_exc, padding=self.h_padding))  # In activ range
    #     exc_intx = self.w_exc(inhibition)
    #     excitation_hat = activ(inhibition + exc_intx * (self.kappa * inhibition + self.gamma))  # Skip connection OR add OR add by self-sim

    #     excitation = (1 - exc_gate) * excitation + exc_gate * excitation_hat
    #     if testmode:
    #         return inhibition, excitation, att_gate
    #     else:
    #         return inhibition, excitation

    ################################################################
    ######################## All group norm m#######################
    ################################################################
    def forward(self, input_, inhibition, excitation,  activ=F.softplus, testmode=False):  # Worked with tanh and softplus
        # Attention gate: filter input_ and excitation
        # att_gate = torch.sigmoid(self.a_w_gate(input_) + self.a_u_gate(excitation))  # Attention Spotlight
        # att_gate = torch.sigmoid(self.a_w_gate(input_) * self.a_u_gate(excitation))  # Attention Spotlight
        if inhibition is None:
            inhibition = self.inh_init(input_)
        if excitation is None:
            excitation = self.exc_init(input_)
        if self.use_attention:
            # att_gate = torch.sigmoid(self.a_w_gate(inhibition) + self.a_u_gate(excitation))  # Attention Spotlight -- MOST RECENT WORKING
            att_gate = self.att_nl(self.gbn[0](self.a_w_gate(input_) + self.a_u_gate(excitation)))  # Attention Spotlight -- MOST RECENT WORKING
        elif not self.use_attention and testmode:
            att_gate = torch.zeros_like(input_)

        # Gate E/I with attention immediately
        if self.use_attention:
            gated_input = input_  # * att_gate  # In activ range
            gated_excitation = att_gate * excitation  # att_gate * excitation
            gated_inhibition = inhibition
            # gated_inhibition = inhibition
        else:
            gated_input = input_
            gated_excitation = excitation
            gated_inhibition = inhibition

        if not self.no_inh:
            # Compute inhibition
            # inh_intx = self.bn[0](F.conv2d(gated_excitation, self.w_inh, padding=self.h_padding))  # in activ range
            inh_intx = self.gbn_r[0](self.w_inh(gated_excitation))  # , self.w_inh, padding=self.h_padding))  # in activ range
            inhibition_hat = activ(input_ - inh_intx * (self.alpha * gated_inhibition + self.mu))
            # inhibition_hat = activ(input_ - activ(inh_intx * (self.alpha * gated_inhibition + self.mu)))

            # Integrate inhibition
            inh_gate = torch.sigmoid(self.gbn[1](self.i_w_gate(gated_input) + self.i_u_gate(gated_inhibition)))
            inhibition = (1 - inh_gate) * inhibition + inh_gate * inhibition_hat  # In activ range
        else:
            inhibition, gated_inhibition = gated_excitation, excitation

        # Pass to excitatory neurons
        # exc_gate = torch.sigmoid(self.e_w_gate(inhibition) + self.e_u_gate(excitation))
        exc_gate = torch.sigmoid(self.gbn[2](self.e_w_gate(gated_inhibition) + self.e_u_gate(gated_excitation)))
        # exc_intx = self.bn[1](F.conv2d(inhibition, self.w_exc, padding=self.h_padding))  # In activ range
        exc_intx = self.gbn_r[1](self.w_exc(inhibition))
        excitation_hat = activ(exc_intx * (self.kappa * inhibition + self.gamma))  # Skip connection OR add OR add by self-sim

        excitation = (1 - exc_gate) * excitation + exc_gate * excitation_hat
        if testmode:
            return inhibition, excitation, att_gate
        else:
            return inhibition, excitation

class FFhGRU(nn.Module):

    def __init__(self, dimensions = 25, input_size=3, timesteps=8, kernel_size=15, jacobian_penalty=False, grad_method='bptt', no_inh=False, \
                 lesion_alpha=False, lesion_mu=False, lesion_gamma=False, lesion_kappa=False, nl=F.softplus, l1=0., output_size=3, \
                 num_rbp_steps=10, LCP=0., jv_penalty_weight=0.002, pre_kernel_size = 7):
        '''
        '''
        super(FFhGRU, self).__init__()
        self.timesteps = timesteps
        self.jacobian_penalty = jacobian_penalty
        self.grad_method = grad_method
        self.hgru_size = dimensions
        self.output_size = output_size
        self.num_rbp_steps = num_rbp_steps
        self.LCP = LCP
        self.jv_penalty_weight = jv_penalty_weight
        if l1 > 0:
            self.preproc = L1(nn.Conv2d(input_size, dimensions, kernel_size=kernel_size, stride=1, padding=kernel_size // 2), weight_decay=l1)
        else:
            self.preproc = nn.Conv2d(input_size, dimensions, kernel_size=kernel_size, stride=1, padding=kernel_size // 2)

        init.xavier_normal_(self.preproc.weight)
        init.constant_(self.preproc.bias, 0)

            # sqzn_pretrained_features = models.squeezenet1_1(pretrained=True).features
            # self.preproc = torch.nn.Sequential()
            # for x in range(len(sqzn_pretrained_features)-1):
            #     print(sqzn_pretrained_features[x])
            #     self.preproc.add_module(str(x), sqzn_pretrained_features[x])

        # self.preproc1 = nn.Conv2d(input_size, dimensions, kernel_size=7, stride=1, padding=7 // 2)
        self.unit1 = hConvGRUCell(
            input_size=input_size,
            hidden_size=self.hgru_size,
            kernel_size=kernel_size,
            use_attention=True,
            no_inh=no_inh,
            l1=l1,
            lesion_alpha=lesion_alpha,
            lesion_mu=lesion_mu,
            lesion_gamma=lesion_gamma,
            lesion_kappa=lesion_kappa,
            timesteps=timesteps)
        self.bn = nn.BatchNorm2d(self.hgru_size, eps=1e-03, affine=False, track_running_stats=True)
        # self.readout_bn = nn.BatchNorm2d(2, eps=1e-03, affine=True, track_running_stats=True)
        # self.readout_conv = nn.Conv2d(dimensions, 2, 1)
        # self.readout_dense = nn.Linear(2, 2)
        self.readout_bn = nn.BatchNorm2d(self.hgru_size, eps=1e-03, affine=True, track_running_stats=True)
        self.readout_dense = nn.Linear(self.hgru_size, self.output_size)
        self.nl = nl


    def forward(self, x, testmode=False, give_timesteps = False):
        # First step: replicate x over the channel dim self.hgru_size times
        xbn = self.preproc(x)
        # print('input map : ', xbn.shape)
        # xbn = self.bn(xbn)  # This might be hurting me...
        xbn = self.nl(xbn)  # TEST TO SEE IF THE NL STABLIZES

        # Extra preproc layer solves multi-color PF... Maybe channel pooling?
        # xbn = self.preproc1(x)
        # xbn = self.bn(xbn)  # This might be hurting me...
        # xbn = self.nl(xbn)  # TEST TO SEE IF THE NL STABLIZES

        # Now run RNN
        x_shape = xbn.shape
        excitation = None
        inhibition = None

        # Loop over frames
        states = []
        gates = []

        time_steps_exc = []

        if self.grad_method == "bptt":
            for t in range(self.timesteps):
                out = self.unit1(
                    input_=xbn,
                    inhibition=inhibition,
                    excitation=excitation,
                    activ=self.nl,
                    testmode=testmode)
                if testmode:
                    inhibition, excitation, gate = out 
                    time_steps_exc.append(excitation)
                    gates.append(gate)  # This should learn to keep the winner
                    states.append(self.readout_conv(excitation))  # This should learn to keep the winner
                else:
                    inhibition, excitation = out 
                    time_steps_exc.append(excitation)
        elif self.grad_method == "rbp":
            with torch.no_grad():
               for t in range(self.timesteps - 1):
                    out = self.unit1(
                        input_=xbn,
                        inhibition=inhibition,
                        excitation=excitation,
                        activ=self.nl,
                        testmode=testmode)
                    if testmode:
                        inhibition, excitation, gate = out
                        gates.append(gate)  # This should learn to keep the winner
                        states.append(self.readout_conv(excitation))  # This should learn to keep the winner
                    else:
                        inhibition, excitation = out
            pen_exc = excitation.detach().requires_grad_()
            last_inh, last_exc = self.unit1(xbn, inhibition=inhibition, excitation=pen_exc, activ=self.nl, testmode=testmode)
            import pdb;pdb.set_trace()
            # Need to attach exc with inh to propoagate grads
            excitation = dummyhgru.apply(pen_exc, last_exc, self.num_rbp_steps)
        else:
            raise NotImplementedError(self.grad_method)

        # output = self.readout_bn(excitation)
        # output = self.readout_conv(output)
        # output = self.readout_conv(excitation)
        # output = excitation




        # output = self.readout_bn(excitation)
        # output = F.avg_pool2d(output, kernel_size=output.size()[2:])
        # output = output.reshape(x_shape[0], -1)
        # output = self.readout_dense(output)
        # pen_type = 'l1'
        # jv_penalty = torch.tensor([1]).float().cuda()
        # if testmode: return output, torch.stack(states, 1), torch.stack(gates, 1)

        # if self.training and self.LCP:
        #     norm_1_vect = torch.ones_like(last_exc)
        #     norm_1_vect.requires_grad = False
        #     jv_prod = torch.autograd.grad(last_exc, pen_exc, grad_outputs=[norm_1_vect], retain_graph=True, create_graph=True, allow_unused=True)[0]
        #     jv_penalty = ((jv_prod - self.LCP).clamp(0) ** 2).sum()
        #     out_dict = {"logits": output, "penalty": jv_penalty * self.jv_penalty_weight}
        #     return out_dict
        # else:
        #     return output

        if give_timesteps:

            return time_steps_exc

        else:
        
            return excitation

"""# **models_hGRU_center**"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
from torchvision import transforms,datasets
# import matplotlib.pyplot as plt
import torch.optim as optim
from torch.nn import init
from tqdm import tqdm_notebook as tqdm
import os
import numpy as np
# import pandas as pd
import cv2
import _pickle as pickle
import math
# from PIL import Image
# from torchsummary import summary
# import time
from torch.utils.tensorboard import SummaryWriter
from torchvision import datasets, models, transforms

from collections import OrderedDict

# import pytorch_lightning as pl
# from sklearn.preprocessing import LabelEncoder
from pytorch_lightning import Trainer
from pytorch_lightning.loggers import WandbLogger

# from utils import Conv2dPad

# from model import FFhGRU

USE_CONFIDENCE_WEIGHTED_POOLING = True

class FFhGRU_pl(pl.LightningModule):
    def __init__(self, base = None):
        super().__init__()
        # self.train_accuracy = pl.metrics.Accuracy()
        # self.val_accuracy = pl.metrics.Accuracy()

        # self.no_timesteps = 3

        self.base = FFhGRU(32, timesteps=8, kernel_size=7, nl=F.softplus, input_size=3, output_size=3, l1=0., pre_kernel_size=7)

        # # Final convolutional layers (conv6 and conv7) to extract semi-dense feature maps
        # self.final_convs = nn.Sequential(
        #     nn.MaxPool2d(kernel_size=2, stride=1, ceil_mode=True),
        #     nn.Conv2d(128, 64, kernel_size=6, stride=1, padding=3),
        #     nn.ReLU(inplace=True),
        #     nn.Dropout(p=0.5),
        #     nn.Conv2d(64, 4 if USE_CONFIDENCE_WEIGHTED_POOLING else 3, kernel_size=1, stride=1),
        #     nn.ReLU(inplace=True)
        # )

        # self.linear = nn.Linear(32, 3)

        self.conv_readout = nn.Conv2d(32, 3, kernel_size=1, stride=1)

        init.xavier_normal_(self.conv_readout.weight)
        init.constant_(self.conv_readout.bias, 0)

        self.min_loss = 1000



    def forward(self, x, return_hidden=False, test_mode = False):

        
        xs = self.base(x)

        if test_mode:

            time_list = []
            pred_list = []

            for i in range(len(xs)):

                x = xs[i]

                x = self.conv_readout(x)

                time_list.append(x)

                # shape = x.shape[2]

                # x = F.avg_pool2d(x, kernel_size=shape)

                # x = x.view(x.shape[0], 3)

                # pred_list.append(x)

            return time_list


        else:
        

            x = self.conv_readout(xs)

            # shape = x.shape[2]

            # x = F.avg_pool2d(x, kernel_size=shape)

            # x = x.view(x.shape[0], 3)

            return x
    
    def normalized_angular_loss(self, estimate, target, reduce=True):
    
        # if len(estimate.shape) == 4:
        #     estimate = F.normalize(estimate, p=2, dim=1)
        #     estimate = estimate.sum((-2, -1))
        # if len(estimate.shape) > 2:
        #     estimate = estimate.view([-1, estimate.shape[-1]])
        # if len(target.shape) > 2:
        #     target = target.view([-1, target.shape[-1]])
        safe_v = 0.999999

        estimate = F.normalize(estimate, p=2, dim=1)
        target = F.normalize(target, p=2, dim=1)
        dot = torch.sum(estimate * target, 1)
        dot = torch.clamp(dot, -safe_v, safe_v)

        angle = torch.acos(dot) * (180 / np.pi)
        if reduce:
            angle = torch.mean(angle)

        return angle

    def angular_loss(self, pred, label):
        safe_v = 0.999999
        dot = torch.clamp(torch.sum(F.normalize(pred, dim=1) * F.normalize(label, dim=1), dim=1), -safe_v, safe_v)
        angle = torch.acos(dot) * (180 / math.pi)
        return torch.mean(angle)
    
    #pytorch lighning functions
    def configure_optimizers(self):
        optimiser = torch.optim.Adam(self.parameters(),lr=0.0001)#, betas=(0.9, 0.999), eps=1e-08, weight_decay=5.7e-5)
        return optimiser
    def training_step(self, batch, batch_idx):
        x, y = batch

        x = x.reshape(-1, 3, 256, 256)
        y = y.reshape(-1, 3, 256, 256)

        pred_img = self(x)

        # loss = self.angular_loss(pred, y)
        # loss = F.mse_loss(pred, y)
        loss = self.angular_loss(pred_img[:,:,128,128], y[:,:,128,128])

        # self.train_accuracy(pred,y)
        self.log('train_loss', loss,on_step=False, on_epoch=True,prog_bar=True)

        return loss 

#     def training_epoch_end(self,output):
#         self.log('train_acc_epoch', self.train_accuracy(),on_step=True, on_epoch=True)
    def validation_step(self, batch, batch_idx):
        test_mode = False
        
        x, y = batch

        x = x.reshape(-1, 3, 256, 256)
        y = y.reshape(-1, 3, 256, 256)

        pred_img = self(x, test_mode = test_mode)
        i = 0
        
        # if test_mode:
        #     if batch_idx == 0:
        #         for xx, yy in zip(x, y):
        #             cv2.imwrite(f'/users/aarjun1/scratch/color_data/check_hGRU_seg_2/in{i}.png', (np.transpose(xx.cpu().numpy(), (1, 2, 0))*199))
        #             cv2.imwrite(f'/users/aarjun1/scratch/color_data/check_hGRU_seg_2/gt{i}.png', (np.transpose(yy.cpu().numpy(), (1, 2, 0))*199))

        #             for j in range(len(preds)):
        #                 pre = preds[j][i]
        #                 cv2.imwrite(f'/users/aarjun1/scratch/color_data/check_hGRU_seg_2/recon_{i}_{j}.png', (np.transpose(pre.cpu().numpy(), (1, 2, 0))*199))
                    
        #             i += 1

        # else:
        #     if batch_idx == 0:
        #         for xx, yy, pre in zip(x, y, pred_img):
        #             cv2.imwrite(f'/content/drive/MyDrive/rendered_images/TPU_dummy/vizz/2in{i}.png', (np.transpose(xx.cpu().numpy(), (1, 2, 0))*199))
        #             cv2.imwrite(f'/content/drive/MyDrive/rendered_images/TPU_dummy/vizz/2gt{i}.png', (np.transpose(yy.cpu().numpy(), (1, 2, 0))*199))
        #             cv2.imwrite(f'/content/drive/MyDrive/rendered_images/TPU_dummy/vizz/2recon_{i}.png', (np.transpose(pre.cpu().numpy(), (1, 2, 0))*199))
                    
        #             i += 1

        # val_loss = self.angular_loss(pred, y)
        if test_mode:
            pred = preds[-1]
            val_loss = self.angular_loss(pred[:,:,128,128], y[:,:,128,128])
        else:
            val_loss = self.angular_loss(pred_img[:,:,128,128], y[:,:,128,128])

        # self.val_accuracy(pred,y)

        self.log('val_loss', val_loss,on_step=False, on_epoch=True,prog_bar=True)
        
        return val_loss

    def validation_epoch_end(self,losses):
        losses = [loss.cpu().numpy() for loss in losses]
        losses = np.array(losses)
        result_summary = OrderedDict()
        # for k in list(cur_score.keys()) + ['loss']:
        result_summary["angular_error" + "_mean"] = np.mean(losses)
        result_summary["angular_error" + "_median"] = np.median(losses)
        result_summary["angular_error" + "_sem"] = np.std(
            losses
        ) / np.sqrt(losses.shape[-1])
        result_summary["angular_error" + "_max"] = np.max(losses)

        # if np.mean(losses) < self.min_loss:
        #     with open('/users/aarjun1/data/aarjun1/color_cnn_FFhGRU_center/data/fold20_loss.pkl', 'wb') as f:
        #         pickle.dump(losses, f, protocol=-1)
        #     self.min_loss = np.mean(losses)

        # print(result_summary)

"""# **Trainer**"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
from torch.nn import init
from torchvision import transforms,datasets
# import matplotlib.pyplot as plt
import torch.optim as optim
from tqdm import tqdm_notebook as tqdm
import os
import numpy as np
# import pandas as pd
import cv2
# from PIL import Image
# from torchsummary import summary
# import time
from torch.utils.tensorboard import SummaryWriter

import pytorch_lightning as pl
# from sklearn.preprocessing import LabelEncoder
from pytorch_lightning import Trainer
from pytorch_lightning.loggers import WandbLogger


from torch.utils.data import random_split, DataLoader, Dataset

from pytorch_lightning.callbacks import ModelCheckpoint

# import dataloaderr
# import models_hGRU_center_GAP
# import models_hGRU_center

def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

if __name__ == '__main__':
    if torch.cuda.is_available():
        n_gpus = torch.cuda.device_count()
        
        print("GPUs detected: = {}".format( torch.cuda.device_count() ) )
        
        for i in range(n_gpus):
            print("_______")
            print( torch.cuda.get_device_name( i ) )
            print("_______")


    # pt_model = model_center_pt.SQUEEZENET_1_1()
    # pt_model = pt_model.load_from_checkpoint('/users/aarjun1/data/aarjun1/color_cnn/checkpoints/unet_vgg-epoch=56-val_loss=2.96.ckpt')

    # base = pt_model.slice

    model = FFhGRU_pl()
    # model = model.load_from_checkpoint('/users/aarjun1/data/aarjun1/color_cnn_FFhGRU_center/checkpoints_8t_AME_GN/unet_vgg-epoch=52-val_loss=2.99.ckpt')
    print(model)
    print('Number of Parameters', count_parameters(model))
    
    
    input_file = 'gs://serrelab/prj_constancy/synthetic_data/input'
    output_file = 'gs://serrelab/prj_constancy/synthetic_data/output'
    sm_file = 'gs://serrelab/prj_constancy/synthetic_data/sm'

    data = blender_loader(input_file, output_file, sm_file)

    checkpoint_callback = ModelCheckpoint(
                            monitor="val_loss",
                            dirpath="data/checkpoints_8t_AME_Xavier_both/",
                            filename="unet_vgg-{epoch:02d}-{val_loss:.2f}",
                            save_top_k=3,
                            mode="min",
                        )

    trainer = pl.Trainer(max_epochs=1000, tpu_cores=8, log_every_n_steps=1, progress_bar_refresh_rate = 1) 
    trainer.fit(model, data) 
    # trainer.test(model, data)

tpu_cores

!sysctl kernel.shmmax

